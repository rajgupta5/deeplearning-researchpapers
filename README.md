# deeplearning-researchpapers


## **Neural Network**
1. Adam - A METHOD FOR STOCHASTIC OPTIMIZATION
[https://arxiv.org/pdf/1412.6980.pdf]

2. Leaky RELU
[https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf]

3. ELU
[https://arxiv.org/pdf/1511.07289.pdf]

4. SELU
[https://arxiv.org/pdf/1706.02515.pdf]

5. GELU
[https://arxiv.org/pdf/1606.08415.pdf]



## _**Computer Vision**_
![CV](https://appsilondatascience.com/assets/uploads/2018/08/types.png)

![CNN Architectures timeline](https://miro.medium.com/max/5788/1*dc07I4_N_IWDJVb6cM-KsQ.png)

### **Image Classification Tasks Architectures**
1. **LeNet5** 
(GradientBased Learning Applied to Document
Recognition)[http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf]
Explanation
[https://www.youtube.com/watch?v=SrhFB3o14yc]


2. **AlexNet**
(ImageNet Classification with Deep Convolutional
Neural Networks)[https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf]
(The History Began from AlexNet: A Comprehensive Survey on Deep Learning Approaches)[https://arxiv.org/pdf/1803.01164.pdf]
Explanation
[https://www.youtube.com/watch?v=Nq3auVtvd9Q]
[https://www.youtube.com/watch?v=JOuHjUumVE8]


3. **VGG16,19**
(VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION)[https://arxiv.org/pdf/1409.1556.pdf]
Explanation
[https://www.youtube.com/watch?v=x9HF1Vhk9Rw&feature=youtu.be]


4. **ResNet**
(Deep Residual Learning for Image Recognition)[https://arxiv.org/pdf/1512.03385.pdf]
Explanation
[https://www.youtube.com/watch?v=GWt6Fu05voI]
[https://www.youtube.com/watch?v=imqN-JlGYb4]

5. **GoogleNet/InceptionNet**
(Going deeper with convolutions)[https://arxiv.org/pdf/1409.4842.pdf]
Explanation
[https://www.youtube.com/watch?v=EfpbbaJ_JG0&t=3404s]

### **Object Detection Tasks Architectures**

![Object Detection](https://bitmovin.com/wp-content/uploads/2019/08/Object_detection_Blog_Image_Q3_19.jpg)


![OD Algos timeline](https://www.mdpi.com/electronics/electronics-09-00583/article_deploy/html/images/electronics-09-00583-g001.png)

6. Selective Search for Object Recognition 
[http://www.huppelen.nl/publications/selectiveSearchDraft.pdf]

7. **Region Proposal R-CNN**
(Rich feature hierarchies for accurate object detection and semantic segmentation)[https://arxiv.org/pdf/1311.2524.pdf]
Explanation
[https://www.youtube.com/watch?v=BzMLBDcUqL0]

8. **Fast R-CNN**
(Fast R-CNN)[https://arxiv.org/pdf/1504.08083.pdf]
Explanation
[https://www.youtube.com/watch?v=BzMLBDcUqL0]

9. **Faster RCNN**
(Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks)[https://arxiv.org/pdf/1506.01497.pdf]
Explanation
[https://www.youtube.com/watch?v=7KBpKJjPSG8]

10. **SSD**
(SSD: Single Shot MultiBox Detector)[https://arxiv.org/pdf/1512.02325.pdf]

11. **YOLO**
(You Only Look Once: Unified, Real-Time Object Detection)
Explanation
[https://www.youtube.com/watch?v=NkM3y3h8X8M]

   11.1 ***YOLOv1***
   [https://arxiv.org/pdf/1506.02640.pdf]
   [https://docs.google.com/presentation/d/1aeRvtKG21KHdD5lg6Hgyhx5rPq_ZOsGjG5rJ1HP7BbA/pub?start=false&loop=false&delayms=3000&slide=id.p]
   [https://deepsystems.ai/reviews]

   11.2 ***YOLOv2***
    [https://pjreddie.com/media/files/papers/YOLO9000.pdf]

   11.3 ***YOLOv3***
    [https://arxiv.org/pdf/1804.02767.pdf]
  
   11.4 ***YOLOv4***
    [https://www.researchgate.net/publication/340883401_YOLOv4_Optimal_Speed_and_Accuracy_of_Object_Detection]
    
12. DETECTRON2
[https://research.fb.com/wp-content/uploads/2019/12/4.-detectron2.pdf]

### **Segmentation Tasks Architectures**

Instance Segementaion

![Segmentation](https://cdn.analyticsvidhya.com/wp-content/uploads/2019/07/feature_image.jpg)

Semantic Segmentation

![Segmentation](https://cdn-images-1.medium.com/max/720/1*CRRcs2eAJIRS8Q-qa6_YMw.gif)


1. **MASK-RCNN**
[https://arxiv.org/pdf/1703.06870.pdf]

### **Object Tracking Tasks Architectures**

![Object Tracking](https://nanonets.com/blog/content/images/2019/07/object_tracker_gif.gif)

1. **DeepSort**



## _**Natural Language Processing**_

![Transformer Based Models](https://miro.medium.com/max/1400/1*bG8dQv1L6o3NRXyCau4Rzg.png)

https://medium.com/@Moscow25/the-best-deep-natural-language-papers-you-should-read-bert-gpt-2-and-looking-forward-1647f4438797


1. (Deep Learning Based Text Classification: A Comprehensive Review)[https://arxiv.org/pdf/2004.03705.pdf]

2. Transformers: Attention is all you need
[https://arxiv.org/pdf/1706.03762.pdf]
Explanation
[https://www.youtube.com/watch?v=iDulhoQ2pro&list=PL1v8zpldgH3pXDttKKp8mlVKDitxsYDAp&index=6]
[http://jalammar.github.io/illustrated-transformer/]

3. Word2Vec - Distributed Representations of Words and Phrases and their Compositionality
[https://arxiv.org/pdf/1310.4546.pdf]
Explanation
[https://www.youtube.com/watch?v=yexR53My2O4]

4. Sequence to Sequence learning with NN 
[https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf]
[https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/]

5. NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE
[https://arxiv.org/pdf/1409.0473.pdf]

6. BLEU: a Method for Automatic Evaluation of Machine Translation
[https://www.aclweb.org/anthology/P02-1040.pdf]

7. Effective Modeling of Encoder-Decoder Architecture for Joint Entity and Relation Extraction
[https://arxiv.org/pdf/1911.09886.pdf]

8. BiDirectional LSTM
[https://arxiv.org/pdf/1508.01991.pdf]

### **NLP Language Models**
1. GPT-1

2. GPT-2 - stands for “Generative Pretrained Transformer 2”: “Generative” means the model was trained to predict (or “generate”) the next token in a sequence of tokens in an unsupervised way
[https://arxiv.org/pdf/2001.08764.pdf]

3. GPT-3 - Language Models are Few-Shot Learners
[https://arxiv.org/pdf/2005.14165.pdf]
Explanation
[https://www.youtube.com/watch?v=SY5PvZrJhLE]

4. BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding
[https://arxiv.org/pdf/1810.04805.pdf]
Explanation
[https://www.youtube.com/watch?v=-9evrZnBorM]

5. ELMO

6. ALBERT - A Lite BERT for Self-supervised Learning of Language Representations
[https://arxiv.org/abs/1909.11942]

7. DistilBERT

8. T5 - Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer
[https://arxiv.org/abs/1910.10683]

9. RoBERTa - A Robustly Optimized BERT Pretraining Approach
[https://arxiv.org/abs/1907.11692]

10. XLNet - Generalized Autoregressive Pretraining for Language Understanding
[https://arxiv.org/abs/1906.08237]

